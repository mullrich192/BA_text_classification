{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "from numpy import nan\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "import gensim as ge\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pyLDAvis\n",
    "from pyLDAvis import gensim\n",
    "from gensim import  models\n",
    "import gensim.corpora as corpora\n",
    "import pyLDAvis.gensim_models\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models import CoherenceModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle \n",
    "\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '/home/ubuntu/ullrich/data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocess train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define connection to db \n",
    "connect_string = 'postgresql+psycopg2://postgres:5050@localhost:5432/postgres'\n",
    "#define sql queries\n",
    "sql_query_agro = 'SELECT * FROM publ.corpus_keywords_agro'\n",
    "sql_query_mesh = 'SELECT * FROM publ.corpus_keywords_mesh'\n",
    "sql_query_class = 'SELECT dbrecordid, class FROM ke_stage.corpus_small'\n",
    "\n",
    "#create engine\n",
    "engine = create_engine(connect_string)\n",
    "#read data as df\n",
    "df_agro = pd.read_sql(sql_query_agro, engine)\n",
    "df_mesh = pd.read_sql(sql_query_mesh, engine)\n",
    "df_class = pd.read_sql(sql_query_class, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge the ids from abs, if title-ID has none-values\n",
    "def join_id(df):\n",
    "    value = df['id1'].fillna(df['id2'])\n",
    "    df['id1'] = value\n",
    "    df = df.drop(['id2'], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agro = join_id(df_agro)\n",
    "df_mesh = join_id(df_mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge both df (agrovoc and mesh)\n",
    "result = pd.merge(df_agro, df_mesh, on=['id1'], how='outer')\n",
    "#create list of all keywords for each document\n",
    "result['keywords'] = result.apply(lambda row: [val for val in row if isinstance(val, list)], axis=1)\n",
    "\n",
    "temp_list =  []\n",
    "for row in result['keywords']:\n",
    "    keywords_list = []\n",
    "    for l in row:\n",
    "        keywords_list.extend(l)\n",
    "    for x in range(len(keywords_list)):\n",
    "        keywords_list[x] = keywords_list[x].lower()\n",
    "    temp_list.append(keywords_list)\n",
    "    \n",
    "result['keywords_all'] = temp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop unnessecary columns and rename\n",
    "result = result.drop(columns=['keywords'])\n",
    "result = result.rename(columns={'id1':'dbrecordid'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge df with classes\n",
    "result = pd.merge(result,df_class, on=['dbrecordid'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete rows with the class 'Rest'\n",
    "result = result[result['class'] != 'Rest']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train model LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into train and test data\n",
    "df_train, df_test = train_test_split(result, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get list of keywords\n",
    "keywords_train = df_train['keywords_all'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 2), (1, 4), (2, 2), (3, 2), (4, 2), (5, 3), (6, 2), (7, 9), (8, 2), (9, 21), (10, 2), (11, 2), (12, 6), (13, 2), (14, 3), (15, 4), (16, 4)], [(0, 4), (17, 14), (18, 10), (19, 2), (20, 6), (21, 8), (22, 3), (23, 4), (24, 4), (25, 2), (26, 4), (27, 12), (28, 1), (29, 2), (30, 2)], [(31, 2), (32, 2), (33, 14), (34, 16), (35, 4), (36, 2), (37, 2), (38, 1), (39, 2), (40, 3), (41, 4), (42, 4), (43, 2), (44, 1), (45, 1), (46, 6), (47, 2), (48, 1), (49, 1), (50, 2), (51, 2)], [(52, 1), (53, 2), (54, 8), (55, 2), (56, 4), (57, 2), (58, 4), (59, 2), (60, 4), (61, 4), (62, 2), (63, 2), (64, 4), (65, 1), (66, 2), (67, 16), (68, 2), (69, 6), (70, 2)], [(71, 1), (72, 2), (73, 2), (74, 6), (75, 4), (76, 2), (77, 3), (78, 2), (79, 4), (80, 12)], [(55, 6), (79, 2), (81, 6), (82, 2), (83, 4), (84, 4), (85, 2), (86, 2), (87, 4), (88, 2), (89, 2), (90, 1), (91, 2), (92, 10)], [(16, 2), (93, 2), (94, 2), (95, 1), (96, 1), (97, 2), (98, 2), (99, 3), (100, 2), (101, 2), (102, 2), (103, 9)], [(55, 14), (72, 20), (79, 2), (99, 1), (104, 2), (105, 3), (106, 2), (107, 3), (108, 2), (109, 2), (110, 2), (111, 2), (112, 6), (113, 3), (114, 2), (115, 6), (116, 2), (117, 2), (118, 2), (119, 2), (120, 2), (121, 4), (122, 2), (123, 6), (124, 2)], [(40, 3), (97, 1), (98, 2), (125, 6), (126, 2), (127, 10), (128, 2), (129, 1), (130, 2), (131, 2), (132, 2), (133, 2), (134, 2), (135, 2), (136, 19), (137, 2), (138, 6), (139, 2), (140, 2), (141, 8), (142, 2), (143, 4), (144, 8), (145, 2), (146, 2), (147, 12)], [(148, 4), (149, 8), (150, 10), (151, 12), (152, 8), (153, 14), (154, 2), (155, 1), (156, 2), (157, 30), (158, 1), (159, 9), (160, 2), (161, 7), (162, 22), (163, 6)]]\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(keywords_train)\n",
    "# Create Corpus\n",
    "keywords_str = keywords_train\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in keywords_str]\n",
    "# View\n",
    "#print(corpus[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.025*\"cells\" + 0.025*\"methods\" + 0.019*\"genes\" + 0.012*\"rats\" + '\n",
      "  '0.010*\"mice\" + 0.009*\"humans\" + 0.009*\"therapeutics\" + 0.007*\"research\" + '\n",
      "  '0.007*\"attention\" + 0.007*\"infection\"'),\n",
      " (1,\n",
      "  '0.019*\"plants\" + 0.016*\"growth\" + 0.013*\"water\" + 0.012*\"economics\" + '\n",
      "  '0.011*\"soil\" + 0.011*\"attention\" + 0.007*\"forests\" + 0.007*\"therapeutics\" + '\n",
      "  '0.007*\"methods\" + 0.007*\"environment\"'),\n",
      " (2,\n",
      "  '0.064*\"patients\" + 0.029*\"therapeutics\" + 0.023*\"methods\" + '\n",
      "  '0.011*\"neoplasms\" + 0.009*\"cells\" + 0.009*\"risk\" + 0.008*\"mortality\" + '\n",
      "  '0.008*\"disease\" + 0.007*\"health\" + 0.007*\"women\"'),\n",
      " (3,\n",
      "  '0.014*\"methods\" + 0.010*\"carbon\" + 0.010*\"attention\" + 0.010*\"research\" + '\n",
      "  '0.010*\"patients\" + 0.008*\"economics\" + 0.007*\"water\" + 0.007*\"time\" + '\n",
      "  '0.007*\"temperature\" + 0.006*\"therapeutics\"')]\n"
     ]
    }
   ],
   "source": [
    "# number of topics\n",
    "num_topics = 4\n",
    "# Build LDA model\n",
    "lda_model = ge.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics\n",
    "                                       )\n",
    "# Print the keywords in the 5 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score:  0.33641594383898077\n",
      "\n",
      "Perplexity:  -7.416605622733863\n"
     ]
    }
   ],
   "source": [
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=keywords_train, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('Coherence Score: ', coherence_lda)\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus)) # a measure of how good the model is. lower the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDAvis_data_filepath = os.path.join(save_path + 'ldavis_prepared_keywords')\n",
    "\n",
    "if 1 == 1:\n",
    "    LDAvis_prepared = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)\n",
    "    with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)\n",
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "pyLDAvis.save_html(LDAvis_prepared, LDAvis_data_filepath + 'keywords_4classes.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save LDA model\n",
    "temp_file = datapath(save_path + 'lda_model_4classes')\n",
    "lda_model.save(temp_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## implementation testdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load LDA model\n",
    "temp_file = datapath(save_path + 'lda_model_4classes')\n",
    "lda = models.ldamodel.LdaModel.load(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get testdata into list\n",
    "keywords_test = df_test['keywords_all'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_id_corpus(data):\n",
    "    # Create Dictionary\n",
    "    id2word = corpora.Dictionary(data)\n",
    "    # Term Document Frequency\n",
    "    corpus = [id2word.doc2bow(text) for text in data]\n",
    "    return corpus, id2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict topic \n",
    "def get_topic(liste, lda):\n",
    "    to_pro = []\n",
    "    corpus, id2w = to_id_corpus(liste)\n",
    "    topic = lda.get_document_topics(corpus, minimum_probability=0.5, minimum_phi_value=None,\n",
    "                                   per_word_topics=False)\n",
    "    for t in topic:\n",
    "            to_pro.append(t)\n",
    "    return to_pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = get_topic(keywords_test, lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split predicted output into topic-number and certainty\n",
    "df_test['topic'] = topics\n",
    "df_test[\"topic\"] = df_test[\"topic\"].astype(\"str\")\n",
    "df_test[\"topic\"] = df_test[\"topic\"].replace(to_replace=r'[^\\d|\\.|\\,]', value='', regex=True)\n",
    "df_test[\"topic\"] = df_test[\"topic\"].replace('', np.nan)\n",
    "df_test[[\"topic\",\"certainty\"]] =  df_test[\"topic\"].apply(lambda x: pd.Series(str(x).split(\",\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop unnessecary columns\n",
    "final_df = df_test.drop(columns=['agro_title', 'agro_abs', 'mesh_title', 'mesh_abs'])\n",
    "final_df = final_df.drop(columns=['keywords_all'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(save_path + 'predicted_LDA_4classes.csv', sep=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
