{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from glob import glob\n",
    "import os\n",
    "import re\n",
    "\n",
    "from multiprocessing import Process\n",
    "import concurrent.futures as cf\n",
    "\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import ARRAY, String\n",
    "\n",
    "from langdetect import detect\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "\n",
    "import pickle\n",
    "from keyword_extraction import DictLU_Extract_Exact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#settings \n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#detect language for column title and abstract\n",
    "def detect_language(row):\n",
    "    try:\n",
    "        \n",
    "        if not isinstance(row, str):\n",
    "            if len(row) != 0:          \n",
    "                row = row[0]\n",
    "            else:\n",
    "                row= \"_\"                    \n",
    "        return detect(row)\n",
    "    \n",
    "    except LangDetectException:\n",
    "        return 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_lang_detection():\n",
    "    #create engine\n",
    "    engine = create_engine('postgresql+psycopg2://postgres:5050@localhost:5432/postgres')\n",
    "    # create a process pool with 12 workers\n",
    "    executor = cf.ProcessPoolExecutor(max_workers=12)\n",
    "\n",
    "    for chunk in pd.read_sql('SELECT dbrecordid, title, abstract FROM ke_stage.ba_corpus_1', engine, chunksize=100000):\n",
    "        df_res = pd.DataFrame()\n",
    "        print('Got df with ' + str(len(chunk)) + ' rows')\n",
    "        for i, row in chunk.iterrows():\n",
    "            # entry point of the program??\n",
    "            #execute function for column title\n",
    "            future_title = executor.submit(detect_language, row['title'])\n",
    "            detected_lang_title = future_title.result()\n",
    "            #execute function for column abstract\n",
    "            future_abs = executor.submit(detect_language, row['abstract'])\n",
    "            detected_lang_abs = future_abs.result()\n",
    "            #append to new dataframe\n",
    "            df_res = df_res.append({'dbrecordid': row['dbrecordid'], 'lang_title': detected_lang_title, 'lang_abs': detected_lang_abs}, ignore_index=True)\n",
    "        df_res.to_sql('corpus_language', engine, schema='ke_stage', chunksize=100000, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_lang_detection()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load file for each language\n",
    "files_MeSH = glob('/home/ubuntu/ullrich/my_code/data/pickle/MeSH/*.p')\n",
    "files_agrovoc = glob('/home/ubuntu/ullrich/my_code/data/pickle/AGROVOC/*.p')\n",
    "\n",
    "def load_file(file_path):\n",
    "    [dicts_lower,dicts_upper] = pickle.load(open(file_path, \"rb\"))\n",
    "    DEE = DictLU_Extract_Exact(dicts_upper,dicts_lower)\n",
    "    return DEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords(file_path, row, col_lang):\n",
    "    for file in file_path:\n",
    "        parts = os.path.split(file)\n",
    "        parts = re.split(r'_|\\.', parts[1])\n",
    "        if parts[2] == 'en':\n",
    "            DEE_en = load_file(file)\n",
    "        elif parts[2] == 'de':\n",
    "            DEE_de = load_file(file)\n",
    "        elif parts[2] == 'fr':\n",
    "            DEE_fr = load_file(file)\n",
    "\n",
    "    dicts = None\n",
    "    if col_lang == 'en':\n",
    "        dicts = DEE_en\n",
    "    elif col_lang == 'de':\n",
    "        dicts = DEE_de\n",
    "    elif col_lang == 'fr':\n",
    "        dicts = DEE_fr\n",
    "    \n",
    "    if dicts is not None:\n",
    "        terms_id = []\n",
    "        terms = []\n",
    "        dicts.full(str(row))\n",
    "        res = dicts.result\n",
    "        for k, v in res.items():\n",
    "            terms_id.extend([str(k)] * v['count'])\n",
    "            terms.extend([str(v['term'])] * v['count'])\n",
    "        return terms_id, terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_keyword_extraction(files, name):\n",
    "    #create engine\n",
    "    engine = create_engine('postgresql+psycopg2://postgres:5050@localhost:5432/postgres')\n",
    "    # create a process pool with 12 workers\n",
    "    executor = cf.ProcessPoolExecutor(max_workers=12)\n",
    "\n",
    "    for chunk in pd.read_sql('SELECT * FROM ke_stage.join_lang LIMIT 10000', engine, chunksize=1000):\n",
    "        df_res = pd.DataFrame()\n",
    "        print('Got df with ' + str(len(chunk)) + ' rows')\n",
    "        for i, row in chunk.iterrows():\n",
    "            # entry point of the program??\n",
    "            #extract keywords for column title\n",
    "            future_title = executor.submit(get_keywords, files, row['title'], row['lang_title'])\n",
    "            result_title = future_title.result()\n",
    "            #extract keywords for column abstract\n",
    "            future_abs = executor.submit(get_keywords, files, row['abstract'], row['lang_abs'])\n",
    "            result_abs = future_abs.result()\n",
    "            if result_abs and result_title is not None:\n",
    "                #append to new dataframe\n",
    "                df_res = df_res.append({'dbrecordid': row['dbrecordid'], name +'_ID_title' : result_title[0] , name + '_title': result_title[1] , name + '_ID_abs' : result_abs[0] , name + '_abs': result_abs[1]}, ignore_index=True)       \n",
    "        df_res.to_sql('corpus_keywords_' + name, engine, schema='ke_stage', chunksize=1000, if_exists='append', index=False, dtype={'dbrecordid': String(), name +'_ID_title': ARRAY(String()), name + '_title': ARRAY(String()), name + '_ID_abs': ARRAY(String()), name + '_abs': ARRAY(String())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got df with 1000 rows\n",
      "Got df with 1000 rows\n",
      "Got df with 1000 rows\n",
      "Got df with 1000 rows\n",
      "Got df with 1000 rows\n",
      "Got df with 1000 rows\n",
      "Got df with 1000 rows\n",
      "Got df with 1000 rows\n",
      "Got df with 1000 rows\n",
      "Got df with 1000 rows\n"
     ]
    }
   ],
   "source": [
    "process_keyword_extraction(files_MeSH, 'MeSH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got df with 1000 rows\n",
      "Got df with 1000 rows\n",
      "Got df with 1000 rows\n",
      "Got df with 1000 rows\n",
      "Got df with 1000 rows\n",
      "Got df with 1000 rows\n",
      "Got df with 1000 rows\n",
      "Got df with 1000 rows\n",
      "Got df with 1000 rows\n",
      "Got df with 1000 rows\n"
     ]
    }
   ],
   "source": [
    "process_keyword_extraction(files_agrovoc, 'AGROVOC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
