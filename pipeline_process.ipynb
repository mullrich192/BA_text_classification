{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "from multiprocessing import Process\n",
    "import concurrent.futures as cf\n",
    "\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "from langdetect import detect\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "\n",
    "import pickle\n",
    "from keyword_extraction import DictLU_Extract_Exact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#settings \n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#detect language for column title and abstract\n",
    "def detect_language(row):\n",
    "    try:\n",
    "        \n",
    "        if not isinstance(row, str):\n",
    "            if len(row) != 0:          \n",
    "                row = row[0]\n",
    "            else:\n",
    "                row= \"_\"                    \n",
    "        return detect(row)\n",
    "    \n",
    "    except LangDetectException:\n",
    "        return 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_lang_detection():\n",
    "    #create engine\n",
    "    engine = create_engine('postgresql+psycopg2://postgres:8580@localhost:5432/postgres')\n",
    "    # create a process pool with 14 workers\n",
    "    executor = cf.ProcessPoolExecutor(max_workers=14)\n",
    "\n",
    "    for chunk in pd.read_sql('SELECT dbrecordid, title, abstract FROM ke_stage.ba_corpus_1 LIMIT 10000', engine, chunksize=1000):\n",
    "        df_res = pd.DataFrame()\n",
    "        print('Got df with ' + str(len(chunk)) + ' rows')\n",
    "        for i, row in chunk.iterrows():\n",
    "            # entry point of the program??\n",
    "            #execute function for column title\n",
    "            future_title = executor.submit(detect_language, row['title'])\n",
    "            detected_lang_title = future_title.result()\n",
    "            #execute function for column abstract\n",
    "            future_abs = executor.submit(detect_language, row['abstract'])\n",
    "            detected_lang_abs = future_abs.result()\n",
    "            #append to new dataframe\n",
    "            df_res = df_res.append({'dbrecordid': row['dbrecordid'], 'lang_title': detected_lang_title, 'lang_abs': detected_lang_abs}, ignore_index=True)\n",
    "        #print(df_res) \n",
    "        #df_res.to_sql('corpus_language', engine, schema='ke_stage', chunksize=1000, if_exists='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_lang_detection()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load file for each language\n",
    "path_terms_de = '/home/ubuntu/ullrich/keyword_extraction/pickle/MeSH_dict_german.p'\n",
    "path_terms_en = '/home/ubuntu/ullrich/keyword_extraction/pickle/MeSH_dict_english.p'\n",
    "path_terms_fr = '/home/ubuntu/ullrich/keyword_extraction/pickle/MeSH_dict_french.p'\n",
    "\n",
    "def load_file (path):\n",
    "    [dicts_lower,dicts_upper] = pickle.load(open(path, \"rb\"))\n",
    "    DEE = DictLU_Extract_Exact(dicts_upper,dicts_lower)\n",
    "    return DEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract keywords for columns title and abstract\n",
    "def get_keywords(row, col_lang):\n",
    "    dicts = None\n",
    "    if col_lang == 'en':\n",
    "        dicts = load_file(path_terms_en)\n",
    "    elif col_lang == 'de':\n",
    "        dicts = load_file(path_terms_de)\n",
    "    elif col_lang == 'fr':\n",
    "        dicts = load_file(path_terms_fr)\n",
    "    \n",
    "    if dicts is not None:\n",
    "        terms = []\n",
    "        dicts.full(str(row))\n",
    "        res = dicts.result\n",
    "        for k, v in res.items():\n",
    "            terms.append(str(v['term']))\n",
    "        return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_keyword_extraction():\n",
    "    #create engine\n",
    "    engine = create_engine('postgresql+psycopg2://postgres:8580@localhost:5432/postgres')\n",
    "    # create a process pool with 14 workers\n",
    "    executor = cf.ProcessPoolExecutor(max_workers=14)\n",
    "\n",
    "    for chunk in pd.read_sql('SELECT * FROM ke_stage.join_lang LIMIT 100', engine, chunksize=10):\n",
    "        df_res = pd.DataFrame()\n",
    "        print('Got df with ' + str(len(chunk)) + ' rows')\n",
    "        for i, row in chunk.iterrows():\n",
    "            # entry point of the program??\n",
    "            #extract keywords for column title\n",
    "            future_title = executor.submit(get_keywords, row['title'], row['lang_title'])\n",
    "            result_title = future_title.result()\n",
    "            #extract keywords for column abstract\n",
    "            future_abs = executor.submit(get_keywords, row['abstract'], row['lang_abs'])\n",
    "            result_abs = future_abs.result()\n",
    "            print(result_abs)\n",
    "            #append to new dataframe\n",
    "            df_res = df_res.append({'dbrecordid': row['dbrecordid'], 'keywords_title': result_title, 'keywords_abs': result_abs}, ignore_index=True)\n",
    "        #print(df_res)  \n",
    "        #df_res.to_sql('corpus_keywords', engine, schema='ke_stage', chunksize=10, if_exists='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_keyword_extraction()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
