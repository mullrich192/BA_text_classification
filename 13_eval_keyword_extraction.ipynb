{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Keyword extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import ARRAY, String\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- join mit corpus small\n",
    "- split with /\n",
    "- vergleiche listen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define connection to db \n",
    "connect_string = 'postgresql+psycopg2://postgres:5050@localhost:5432/postgres'\n",
    "#define sql queries\n",
    "query_corpus = 'SELECT dbrecordid, \"MeSH_title\", \"MeSH_abs\" FROM ke_stage.\"corpus_keywords_MeSH\"'\n",
    "query_golden = 'SELECT * FROM ke_stage.corpus_small_key_eval LIMIT 30000000'\n",
    "\n",
    "#create engine\n",
    "engine = create_engine(connect_string)\n",
    "#read data as df\n",
    "df_corpus = pd.read_sql(query_corpus, engine)\n",
    "df_golden = pd.read_sql(query_golden, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.merge(df_corpus, df_golden, on=['dbrecordid'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('/home/ubuntu/ullrich/my_code/data/eval_keywords.csv', sep=',', encoding='utf-8')\n",
    "#result = pd.read_csv('/home/ubuntu/ullrich/my_code/data/eval_keywords.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = result[:5000] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the main words\n",
    "def clean_df(df):\n",
    "    res = [] \n",
    "    for row in df['mesh']:\n",
    "        temp = []   \n",
    "        for item in row:\n",
    "            item = re.sub(r'\\/(.*)$','', item)\n",
    "            if ',' in item:\n",
    "                sublist = item.split(',')\n",
    "                sublist = [split_item.lower().strip() for split_item in sublist]\n",
    "                temp.extend(sublist)\n",
    "            else:\n",
    "                temp.append(item.lower())\n",
    "        res.append(temp)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace nan-values with empty list\n",
    "def replace_nan(df, column):\n",
    "    df[column] = df[column].apply(lambda d: d if isinstance(d, list) else [])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['clean_mesh'] = clean_df(result)\n",
    "result = replace_nan(result,'clean_mesh')\n",
    "result = replace_nan(result,'MeSH_title')\n",
    "result = replace_nan(result,'MeSH_abs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18445/1118950650.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['clean_mesh'] = clean_df(test)\n",
      "/tmp/ipykernel_18445/2840413892.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = df[column].apply(lambda d: d if isinstance(d, list) else [])\n",
      "/tmp/ipykernel_18445/2840413892.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = df[column].apply(lambda d: d if isinstance(d, list) else [])\n",
      "/tmp/ipykernel_18445/2840413892.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = df[column].apply(lambda d: d if isinstance(d, list) else [])\n"
     ]
    }
   ],
   "source": [
    "test['clean_mesh'] = clean_df(test)\n",
    "test = replace_nan(test,'clean_mesh')\n",
    "test = replace_nan(test,'MeSH_title')\n",
    "test = replace_nan(test,'MeSH_abs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get list with all keywords\n",
    "auto_keywords = list(result['MeSH_title'] + result['MeSH_abs'])\n",
    "true_keywords = result['clean_mesh'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get list with all keywords\n",
    "auto_keywords = list(test['MeSH_title'] + test['MeSH_abs'])\n",
    "true_keywords = test['clean_mesh'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(liste):\n",
    "    unique_lists = []\n",
    "    for sublist in liste:\n",
    "        unique_sublist = list(set(sublist))\n",
    "        unique_lists.append(unique_sublist)\n",
    "    return unique_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove duplicates\n",
    "auto_keywords = remove_duplicates(auto_keywords)\n",
    "true_keywords = remove_duplicates(true_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def pre_re(true_list, predicted_list):\n",
    "    # Berechnung der true positives (TP), false positives (FP) und false negatives (FN)\n",
    "    tp = len([1 for true, pred in zip(true_list, predicted_list) if pred in true])\n",
    "    fp = len([1 for pred in predicted_list if pred not in true_list])\n",
    "    fn = len([1 for true, pred in zip(true_list, predicted_list) if true not in pred])\n",
    "    \n",
    "    # Berechnung des Precision-Scores\n",
    "    precision = tp / (tp + fp)\n",
    "\n",
    "    # Berechnung des Recall-Scores\n",
    "    recall = tp / (tp + fn)\n",
    "\n",
    "    # Berechnung des F1-Scores\n",
    "    #f1 = 2 * (int(precision) * int(recall)) / (int(precision) + int(recall))\n",
    "\n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.0\n",
      "recall 0.0\n"
     ]
    }
   ],
   "source": [
    "# Berechnung des F1-Scores\n",
    "precision, recall = pre_re(true_keywords, auto_keywords)\n",
    "\n",
    "#print(\"F1-Score:\", f1_score)\n",
    "print(\"precision:\", precision)\n",
    "print('recall', recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "auto_keywords = mlb.fit([x for x in auto_keywords])\n",
    "true_keywords = mlb.fit([x for x in true_keywords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculation(true_list, pred_list):\n",
    "    result = [] \n",
    "    tmp = {}\n",
    "    trues = [item in true_list for item in pred_list]\n",
    "    f1 = f1_score(trues, [True] * len(trues), average='weighted')\n",
    "    #result.append(tmp)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0004433497536945813"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculation(true_keywords,auto_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision_recall(expected, predicted):\n",
    "    tp = 0  # True Positives\n",
    "    fp = 0  # False Positives\n",
    "    fn = 0  # False Negatives\n",
    "\n",
    "    for i in range(len(expected)):\n",
    "        for j in range(len(expected[i])):\n",
    "            if expected[i][j] in predicted[i]:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "\n",
    "    for i in range(len(predicted)):\n",
    "        for j in range(len(predicted[i])):\n",
    "            if predicted[i][j] not in expected[i]:\n",
    "                fp += 1\n",
    "\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "\n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision_recall(expected, predicted):\n",
    "    expected = [x for x in expected]\n",
    "    predicted = [x for x in expected]  \n",
    "    tp = len(set(expected) & set(predicted))  # True Positives\n",
    "    fp = len(predicted) - tp  # False Positives\n",
    "    fn = len(expected) - tp  # False Negatives\n",
    "\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "\n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.07772199560715407\n",
      "Recall: 0.17746731148128247\n"
     ]
    }
   ],
   "source": [
    "precision, recall = calculate_precision_recall(true_keywords, auto_keywords)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- nur Englische Terme wurden annotiert?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_acc_score(combinations, pred_class):\n",
    "    result = [] \n",
    "    total_count = len(pred_class)\n",
    "    match_count = 0\n",
    "    for item in pred_class:\n",
    "        if item in combinations:\n",
    "            match_count = match_count + 1\n",
    "            print(item)\n",
    "    #print(match_count)        \n",
    "    accuracy = match_count / total_count\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
